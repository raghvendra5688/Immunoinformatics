{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04818fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm\n",
    "import catboost\n",
    "from sklearn import ensemble\n",
    "from sklearn import dummy\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.fixes import loguniform\n",
    "import scipy\n",
    "import argparse\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn.functional import softmax, relu, selu, elu\n",
    "import torch.nn.init as init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import inspect\n",
    "import random\n",
    "import math\n",
    "from dl_model_architecture import NN_Encoder, CNN_Encoder, LSTM_Encoder, Seq2Func, init_weights, count_parameters, training, evaluation, epoch_time\n",
    "\n",
    "from misc import save_model, load_model, regression_results, grid_search_cv, calculate_regression_metrics, supervised_learning_steps, get_CV_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b55a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the environment\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "st = random.getstate()\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.is_available()\n",
    "cudaid = int(0)\n",
    "DEVICE = torch.device(\"cuda:%d\" % (cudaid) if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44cb12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data for your choice: Canonical SMILES + Cell Line Info\n",
    "print(\"Loaded training file\")\n",
    "big_train_df = pd.read_pickle(\"../Data/Training_Set_Var_with_Drug_Embedding_Cell_Info.pkl\",compression=\"zip\")\n",
    "big_test_df = pd.read_pickle(\"../Data/Test_Set_Var_with_Drug_Embedding_Cell_Info.pkl\",compression=\"zip\")\n",
    "total_length = len(big_train_df.columns)\n",
    "\n",
    "metadata_X_train,X_train, Y_train = big_train_df.loc[:,[\"dbgap_rnaseq_sample\",\"inhibitor\"]], big_train_df.iloc[:,[2,1,4]+[*range(288,total_length,1)]], big_train_df[\"auc\"].to_numpy().flatten()\n",
    "metadata_X_test,X_test, Y_test = big_test_df.loc[:,[\"dbgap_rnaseq_sample\",\"inhibitor\"]], big_test_df.iloc[:,[2,1,4]+[*range(288,total_length,1)]], big_test_df[\"auc\"].to_numpy().flatten()\n",
    "\n",
    "#Keep only numeric training and test set and those which have no Nans\n",
    "X_train_numerics_only = X_train.select_dtypes(include=np.number)\n",
    "X_test_numerics_only = X_test[X_train_numerics_only.columns]\n",
    "print(\"Shape of training set after removing non-numeric cols\")\n",
    "print(X_train_numerics_only.shape)\n",
    "print(X_test_numerics_only.shape)\n",
    "\n",
    "nan_cols = [i for i in X_train_numerics_only.columns if X_train_numerics_only[i].isnull().any()]\n",
    "rev_X_train = X_train_numerics_only.drop(nan_cols,axis=1)\n",
    "rev_X_test = X_test_numerics_only.drop(nan_cols,axis=1)\n",
    "print(\"Shape of training set after removing cols with NaNs\")\n",
    "print(rev_X_train.shape)\n",
    "print(rev_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a8d62",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#Load the tokenizer and tokenize SMILES using Vocab from DeepChem\n",
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "#tokenizer = SmilesTokenizer(\"/home/brc05/TII/Projects/Immunoinformatics/Data/vocab.txt\")\n",
    "tokenizer = SmilesTokenizer('/home/raghvendra/TII/Projects/Raghav/Immunoinformatics/Data/vocab.txt')\n",
    "\n",
    "max_smiles_length=150\n",
    "def encode_to_indices(x):\n",
    "    return(torch.tensor(tokenizer.encode(x,max_length=max_smiles_length,padding=\"max_length\")))\n",
    "\n",
    "#Get the list of training smiles\n",
    "X_train_smiles = X_train[\"CanonicalSMILES\"].tolist()\n",
    "X_train_smiles_encoded = [encode_to_indices(x) for x in X_train_smiles]\n",
    "X_test_smiles = X_test[\"CanonicalSMILES\"].tolist()\n",
    "X_test_smiles_encoded = [encode_to_indices(x) for x in X_test_smiles]\n",
    "\n",
    "#Convert train and test smiles to stack of tensors\n",
    "X_train_smiles_encoded = torch.stack(X_train_smiles_encoded)\n",
    "X_test_smiles_encoded = torch.stack(X_test_smiles_encoded)\n",
    "\n",
    "#Convert the cell line info into scaled vectors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_copy = scaler.fit_transform(rev_X_train)\n",
    "X_test_copy = scaler.transform(rev_X_test)\n",
    "\n",
    "#Create the training and test tensor datasets\n",
    "train = data_utils.TensorDataset(X_train_smiles_encoded, torch.Tensor(np.array(X_train_copy)),torch.Tensor(np.array(Y_train)))\n",
    "test = data_utils.TensorDataset(X_test_smiles_encoded, torch.Tensor(np.array(X_test_copy)),torch.Tensor(np.array(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38313a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    #Split the data into 0.8 for training and rest for validation stuff\n",
    "    BATCH_SIZE = 4096\n",
    "    train_dataset, valid_dataset = data_utils.random_split(train, [0.8, 0.2], generator = torch.Generator().manual_seed(i*42))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    #Build model parameters\n",
    "    CELL_INPUT_DIM = X_train_copy.shape[1]\n",
    "    CELL_OUT_DIM = 256\n",
    "    CELL_HID_DIMS = [1024,512]\n",
    "\n",
    "    SMILES_INPUT_DIM = tokenizer.vocab_size\n",
    "    SMILES_ENC_EMB_DIM = 128\n",
    "    SMILES_HID_DIM = 256\n",
    "    SMILES_OUT_DIM = 256\n",
    "\n",
    "    HID_DIM = 128\n",
    "    OUT_DIM = 1\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    cell_enc = NN_Encoder(CELL_INPUT_DIM, CELL_OUT_DIM, CELL_HID_DIMS, DROPOUT)\n",
    "    smiles_enc = LSTM_Encoder(SMILES_INPUT_DIM, SMILES_ENC_EMB_DIM, SMILES_HID_DIM, SMILES_OUT_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "    #Make the model\n",
    "    model = Seq2Func(cell_enc, smiles_enc, HID_DIM, OUT_DIM, DROPOUT, device=DEVICE).to(DEVICE)\n",
    "    print(\"Total parameters in model are: \",count_parameters(model))\n",
    "    model.apply(init_weights)\n",
    "    print(model)\n",
    "    \n",
    "    #Model training criterion\n",
    "    optimizer = optim.Adam(model.parameters(),weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    \n",
    "    #Start training the model\n",
    "    outputfile_model = '../Models/lstm_models/lstm_supervised_checkpoint_'+str(i)+'.pt'\n",
    "    N_EPOCHS = 5000\n",
    "    CLIP = 1\n",
    "    counter = 0\n",
    "    patience = 1000\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if (counter<patience):\n",
    "            print(\"Counter Id: \",str(counter))\n",
    "            start_time = time.time()\n",
    "            train_loss = training(model, train_loader, optimizer, criterion, CLIP, DEVICE)\n",
    "            valid_loss = evaluation(model, valid_loader, criterion, DEVICE)\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            \n",
    "            train_loss_list.append(train_loss)\n",
    "            valid_loss_list.append(valid_loss)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                counter = 0\n",
    "                print(\"Current Val. Loss: %.3f better than prev Val. Loss: %.3f \" %(valid_loss,best_valid_loss))\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), outputfile_model)\n",
    "        else:\n",
    "            counter+=1\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "    if (torch.cuda.is_available()):\n",
    "        model.load_state_dict(torch.load(outputfile_model))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(outputfile_model,map_location=torch.device('cpu')))\n",
    "        \n",
    "    valid_loss = evaluation(model, valid_loader, criterion, DEVICE)\n",
    "    print(f'| Best Valid Loss: {valid_loss:.3f}')\n",
    "\n",
    "    test_loss = evaluation(model, test_loader, criterion, DEVICE)\n",
    "    print(f'| Test Loss: {test_loss: .3f}')\n",
    "\n",
    "    fout_filename = \"../Models/lstm_models/lstm_supervised_\"+str(i)+\"_loss_plot.csv\"\n",
    "    fout=open(fout_filename,\"w\")\n",
    "    for j in range(len(train_loss_list)):\n",
    "        outputstring = str(train_loss_list[j])+\",\"+str(valid_loss_list[j])+\"\\n\"\n",
    "        fout.write(outputstring)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb385c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
