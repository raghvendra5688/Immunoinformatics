{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04818fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra/anaconda3/envs/BeatAML/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.utils.data as data_utils\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import relu,leaky_relu\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "from torch_geometric import data as DATA\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from math import sqrt\n",
    "from rdkit.Chem import AllChem\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import argparse\n",
    "from dl_model_architecture import NN_Encoder, GATNet, CNN_Encoder, LSTM_Encoder, Seq2Func_Net, init_weights, count_parameters, training, training_net, evaluation, evaluation_net, epoch_time\n",
    "\n",
    "from misc import save_model, load_model, regression_results, grid_search_cv, calculate_regression_metrics, supervised_learning_steps, get_CV_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e889d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert SMILES to graph representation\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    if(mol is None):\n",
    "        return None\n",
    "    else:\n",
    "        c_size = mol.GetNumAtoms()\n",
    "        features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            feature = atom_features(atom)\n",
    "            features.append( feature / sum(feature) )\n",
    "\n",
    "        edges = []\n",
    "        for bond in mol.GetBonds():\n",
    "            edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "        g = nx.Graph(edges).to_directed()\n",
    "        edge_index = []\n",
    "        for e1, e2 in g.edges:\n",
    "            edge_index.append([e1, e2])\n",
    "        \n",
    "        return c_size, features, edge_index\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    [atom.GetIsAromatic()])\n",
    "\n",
    "def get_smiles_func(smiles, cell_features, labels):\n",
    "    #Create smiles graphs\n",
    "    data_list=[]\n",
    "    for i in range(0,len(smiles)):\n",
    "        smile = smiles[i]\n",
    "        label = labels[i]\n",
    "        cell_feature = cell_features[i]\n",
    "        g = smile_to_graph(smile)\n",
    "        if(g is None):\n",
    "            print(smile)\n",
    "            none_smiles.append(smile)\n",
    "        else:\n",
    "            c_size, features, edge_index = g[0],g[1],g[2]\n",
    "            \n",
    "            GCNData = DATA.Data(x=torch.FloatTensor(features),\n",
    "                                    edge_index=torch.LongTensor(edge_index).transpose(1, 0),\n",
    "                                    y=torch.FloatTensor([label]))\n",
    "            GCNData.cell_src = torch.FloatTensor(cell_feature)\n",
    "            GCNData.__setitem__('c_size', torch.LongTensor([c_size]))\n",
    "            data_list.append(GCNData)\n",
    "    return(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b55a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Setting up the environment\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "st = random.getstate()\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.is_available()\n",
    "cudaid = int(0)\n",
    "DEVICE = torch.device(\"cuda:%d\" % (cudaid) if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44cb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training file\n",
      "Shape of training set after removing non-numeric cols\n",
      "(34387, 1322)\n",
      "(19184, 1322)\n",
      "Shape of training set after removing cols with NaNs\n",
      "(34387, 1309)\n",
      "(19184, 1309)\n"
     ]
    }
   ],
   "source": [
    "#Get the data for your choice: Canonical SMILES + Cell Line Info\n",
    "print(\"Loaded training file\")\n",
    "big_train_df = pd.read_pickle(\"../Data/Training_Set_Var_with_Drug_Embedding_Cell_Info.pkl\",compression=\"zip\")\n",
    "big_test_df = pd.read_pickle(\"../Data/Test_Set_Var_with_Drug_Embedding_Cell_Info.pkl\",compression=\"zip\")\n",
    "total_length = len(big_train_df.columns)\n",
    "\n",
    "metadata_X_train,X_train, Y_train = big_train_df.loc[:,[\"dbgap_rnaseq_sample\",\"inhibitor\"]], big_train_df.iloc[:,[2,1,4]+[*range(288,total_length,1)]], big_train_df[\"auc\"].to_numpy().flatten()\n",
    "metadata_X_test,X_test, Y_test = big_test_df.loc[:,[\"dbgap_rnaseq_sample\",\"inhibitor\"]], big_test_df.iloc[:,[2,1,4]+[*range(288,total_length,1)]], big_test_df[\"auc\"].to_numpy().flatten()\n",
    "\n",
    "#Keep only numeric training and test set and those which have no Nans\n",
    "X_train_numerics_only = X_train.select_dtypes(include=np.number)\n",
    "X_test_numerics_only = X_test[X_train_numerics_only.columns]\n",
    "print(\"Shape of training set after removing non-numeric cols\")\n",
    "print(X_train_numerics_only.shape)\n",
    "print(X_test_numerics_only.shape)\n",
    "\n",
    "nan_cols = [i for i in X_train_numerics_only.columns if X_train_numerics_only[i].isnull().any()]\n",
    "rev_X_train = X_train_numerics_only.drop(nan_cols,axis=1)\n",
    "rev_X_test = X_test_numerics_only.drop(nan_cols,axis=1)\n",
    "print(\"Shape of training set after removing cols with NaNs\")\n",
    "print(rev_X_train.shape)\n",
    "print(rev_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265a8d62",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24084/2320685607.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  GCNData = DATA.Data(x=torch.FloatTensor(features),\n"
     ]
    }
   ],
   "source": [
    "#Convert the cell line info into scaled vectors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_copy = scaler.fit_transform(rev_X_train)\n",
    "X_test_copy = scaler.transform(rev_X_test)\n",
    "\n",
    "#Get the list of training smiles\n",
    "X_train_smiles = X_train[\"CanonicalSMILES\"].tolist()\n",
    "X_train_smiles_graphs = get_smiles_func(X_train_smiles,X_train_copy,Y_train)\n",
    "X_test_smiles = X_test[\"CanonicalSMILES\"].tolist()\n",
    "X_test_smiles_graphs = get_smiles_func(X_test_smiles,X_test_copy,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38313a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model are:  2373901\n",
      "Counter Id:  0\n",
      "Current Val. Loss: 6846.814 better than prev Val. Loss: inf \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ../Models/gat_models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Val. Loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m better than prev Val. Loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(valid_loss,best_valid_loss))\n\u001b[1;32m     60\u001b[0m         best_valid_loss \u001b[38;5;241m=\u001b[39m valid_loss\n\u001b[0;32m---> 61\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputfile_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/BeatAML/lib/python3.11/site-packages/torch/serialization.py:620\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    621\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    622\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BeatAML/lib/python3.11/site-packages/torch/serialization.py:494\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/BeatAML/lib/python3.11/site-packages/torch/serialization.py:465\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ../Models/gat_models does not exist."
     ]
    }
   ],
   "source": [
    "N_dim = rev_X_train.shape[1]\n",
    "for i in range(0,10):\n",
    "    #Split the data into 0.8 for training and rest for validation stuff\n",
    "    BATCH_SIZE = 256\n",
    "    train_dataset, valid_dataset = data_utils.random_split(X_train_smiles_graphs, [0.8, 0.2], generator = torch.Generator().manual_seed(i*42))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(X_test_smiles_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "    #Build model parameters\n",
    "    CELL_INPUT_DIM = X_train_copy.shape[1]\n",
    "    CELL_OUT_DIM = 256\n",
    "    CELL_HID_DIMS = [1024,512]\n",
    "\n",
    "    SMILES_INPUT_DIM = 78\n",
    "    SMILES_N_HEAD = 2\n",
    "    SMILES_HID_DIM = 256\n",
    "    SMILES_OUT_DIM = 256\n",
    "\n",
    "    HID_DIM = 128\n",
    "    OUT_DIM = 1\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    cell_enc = NN_Encoder(CELL_INPUT_DIM, CELL_OUT_DIM, CELL_HID_DIMS, DROPOUT)\n",
    "    smiles_enc = GATNet(SMILES_INPUT_DIM, SMILES_N_HEAD, SMILES_HID_DIM, SMILES_OUT_DIM, DROPOUT)\n",
    "\n",
    "    #Make the model\n",
    "    model = Seq2Func_Net(cell_enc, smiles_enc, HID_DIM, OUT_DIM, DROPOUT, device=DEVICE).to(DEVICE)\n",
    "    print(\"Total parameters in model are: \",count_parameters(model))\n",
    "    model.apply(init_weights)\n",
    "    #print(model)\n",
    "    \n",
    "    #Model training criterion\n",
    "    optimizer = optim.Adam(model.parameters(),weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    \n",
    "    #Start training the model\n",
    "    outputfile_model = '../Models/gat_models/gat_supervised_checkpoint_'+str(i)+'.pt'\n",
    "    N_EPOCHS = 5000\n",
    "    CLIP = 1\n",
    "    counter = 0\n",
    "    patience = 1000\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if (counter<patience):\n",
    "            print(\"Counter Id: \",str(counter))\n",
    "            start_time = time.time()\n",
    "            train_loss = training_net(model, train_loader, optimizer, criterion, N_dim, CLIP, DEVICE)\n",
    "            valid_loss = evaluation_net(model, valid_loader, criterion, N_dim, DEVICE)\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            \n",
    "            train_loss_list.append(train_loss)\n",
    "            valid_loss_list.append(valid_loss)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                counter = 0\n",
    "                print(\"Current Val. Loss: %.3f better than prev Val. Loss: %.3f \" %(valid_loss,best_valid_loss))\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), outputfile_model)\n",
    "        else:\n",
    "            counter+=1\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "    if (torch.cuda.is_available()):\n",
    "        model.load_state_dict(torch.load(outputfile_model))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(outputfile_model,map_location=torch.device('cpu')))\n",
    "        \n",
    "    valid_loss = evaluation_net(model, valid_loader, criterion, N_dim, DEVICE)\n",
    "    print(f'| Best Valid Loss: {valid_loss:.3f}')\n",
    "\n",
    "    test_loss = evaluation_net(model, test_loader, criterion, N_dim, DEVICE)\n",
    "    print(f'| Test Loss: {test_loss: .3f}')\n",
    "\n",
    "    fout_filename = \"../Models/gat_models/gat_supervised_\"+str(i)+\"_loss_plot.csv\"\n",
    "    fout=open(fout_filename,\"w\")\n",
    "    for j in range(len(train_loss_list)):\n",
    "        outputstring = str(train_loss_list[j])+\",\"+str(valid_loss_list[j])+\"\\n\"\n",
    "        fout.write(outputstring)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb385c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
