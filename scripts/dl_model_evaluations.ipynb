{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5371406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra/anaconda3/envs/BeatAML/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.utils.data as data_utils\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import relu,leaky_relu\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "from torch_geometric import data as DATA\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "#from torch.utils.data import DataLoader\n",
    "from math import sqrt\n",
    "from rdkit.Chem import AllChem\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import argparse\n",
    "from dl_model_architecture import NN_Encoder, GATNet, CNN_Encoder, LSTM_Encoder, Seq2Func, Seq2Func_Net, init_weights, count_parameters, training, training_net, evaluation, evaluation_net, epoch_time, evaluation_net_performance, evaluation_performance\n",
    "\n",
    "from misc import save_model, load_model, regression_results, grid_search_cv, calculate_regression_metrics, supervised_learning_steps, get_CV_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066bb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert SMILES to graph representation\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    if(mol is None):\n",
    "        return None\n",
    "    else:\n",
    "        c_size = mol.GetNumAtoms()\n",
    "        features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            feature = atom_features(atom)\n",
    "            features.append( feature / sum(feature) )\n",
    "\n",
    "        edges = []\n",
    "        for bond in mol.GetBonds():\n",
    "            edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "        g = nx.Graph(edges).to_directed()\n",
    "        edge_index = []\n",
    "        for e1, e2 in g.edges:\n",
    "            edge_index.append([e1, e2])\n",
    "        \n",
    "        return c_size, features, edge_index\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    [atom.GetIsAromatic()])\n",
    "\n",
    "def get_smiles_func(smiles, cell_features, labels):\n",
    "    #Create smiles graphs\n",
    "    data_list=[]\n",
    "    for i in range(0,len(smiles)):\n",
    "        smile = smiles[i]\n",
    "        label = labels[i]\n",
    "        cell_feature = cell_features[i]\n",
    "        g = smile_to_graph(smile)\n",
    "        if(g is None):\n",
    "            print(smile)\n",
    "            none_smiles.append(smile)\n",
    "        else:\n",
    "            c_size, features, edge_index = g[0],g[1],g[2]\n",
    "            \n",
    "            GCNData = DATA.Data(x=torch.FloatTensor(features),\n",
    "                                    edge_index=torch.LongTensor(edge_index).transpose(1, 0),\n",
    "                                    y=torch.FloatTensor([label]))\n",
    "            GCNData.cell_src = torch.FloatTensor(cell_feature)\n",
    "            GCNData.__setitem__('c_size', torch.LongTensor([c_size]))\n",
    "            data_list.append(GCNData)\n",
    "    return(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d743a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Setting up the environment\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "st = random.getstate()\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.is_available()\n",
    "cudaid = int(0)\n",
    "DEVICE = torch.device(\"cuda:%d\" % (cudaid) if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3ac1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training file\n",
      "Shape of training set after removing non-numeric cols\n",
      "(34387, 1322)\n",
      "(19184, 1322)\n",
      "Shape of training set after removing cols with NaNs\n",
      "(34387, 1309)\n",
      "(19184, 1309)\n"
     ]
    }
   ],
   "source": [
    "#Get the data for your choice: Canonical SMILES + Cell Line Info\n",
    "print(\"Loaded training file\")\n",
    "big_train_df = pd.read_pickle(\"../Data/Training_Set_Var_with_Drug_Embedding_Cell_Info.pkl\",compression=\"zip\")\n",
    "big_test_df = pd.read_pickle(\"../Data/Test_Set_Var_with_Drug_Embedding_Cell_Info.pkl\",compression=\"zip\")\n",
    "total_length = len(big_train_df.columns)\n",
    "\n",
    "metadata_X_train,X_train, Y_train = big_train_df.loc[:,[\"dbgap_rnaseq_sample\",\"inhibitor\"]], big_train_df.iloc[:,[2,1,4]+[*range(288,total_length,1)]], big_train_df[\"auc\"].to_numpy().flatten()\n",
    "metadata_X_test,X_test, Y_test = big_test_df.loc[:,[\"dbgap_rnaseq_sample\",\"inhibitor\"]], big_test_df.iloc[:,[2,1,4]+[*range(288,total_length,1)]], big_test_df[\"auc\"].to_numpy().flatten()\n",
    "\n",
    "#Keep only numeric training and test set and those which have no Nans\n",
    "X_train_numerics_only = X_train.select_dtypes(include=np.number)\n",
    "X_test_numerics_only = X_test[X_train_numerics_only.columns]\n",
    "print(\"Shape of training set after removing non-numeric cols\")\n",
    "print(X_train_numerics_only.shape)\n",
    "print(X_test_numerics_only.shape)\n",
    "\n",
    "nan_cols = [i for i in X_train_numerics_only.columns if X_train_numerics_only[i].isnull().any()]\n",
    "rev_X_train = X_train_numerics_only.drop(nan_cols,axis=1)\n",
    "rev_X_test = X_test_numerics_only.drop(nan_cols,axis=1)\n",
    "print(\"Shape of training set after removing cols with NaNs\")\n",
    "print(rev_X_train.shape)\n",
    "print(rev_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c3dc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n",
      "/tmp/ipykernel_15889/2320685607.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  GCNData = DATA.Data(x=torch.FloatTensor(features),\n"
     ]
    }
   ],
   "source": [
    "#Load the tokenizer and tokenize SMILES using Vocab from DeepChem\n",
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "tokenizer = SmilesTokenizer(\"/home/raghvendra/TII/Projects/Raghav/Immunoinformatics/Data/vocab.txt\")\n",
    "\n",
    "max_smiles_length=150\n",
    "def encode_to_indices(x):\n",
    "    return(torch.tensor(tokenizer.encode(x,max_length=max_smiles_length,padding=\"max_length\")))\n",
    "\n",
    "#Get the list of training smiles\n",
    "X_train_smiles = X_train[\"CanonicalSMILES\"].tolist()\n",
    "X_train_smiles_encoded = [encode_to_indices(x) for x in X_train_smiles]\n",
    "X_test_smiles = X_test[\"CanonicalSMILES\"].tolist()\n",
    "X_test_smiles_encoded = [encode_to_indices(x) for x in X_test_smiles]\n",
    "\n",
    "#Convert train and test smiles to stack of tensors\n",
    "X_train_smiles_encoded = torch.stack(X_train_smiles_encoded)\n",
    "X_test_smiles_encoded = torch.stack(X_test_smiles_encoded)\n",
    "\n",
    "#Convert the cell line info into scaled vectors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_copy = scaler.fit_transform(rev_X_train)\n",
    "X_test_copy = scaler.transform(rev_X_test)\n",
    "\n",
    "#Get the list of training smiles\n",
    "X_train_smiles = X_train[\"CanonicalSMILES\"].tolist()\n",
    "X_train_smiles_graphs = get_smiles_func(X_train_smiles,X_train_copy,Y_train)\n",
    "X_test_smiles = X_test[\"CanonicalSMILES\"].tolist()\n",
    "X_test_smiles_graphs = get_smiles_func(X_test_smiles,X_test_copy,Y_test)\n",
    "\n",
    "#Create the training and test tensor datasets\n",
    "train = data_utils.TensorDataset(X_train_smiles_encoded, torch.Tensor(np.array(X_train_copy)),torch.Tensor(np.array(Y_train)))\n",
    "test = data_utils.TensorDataset(X_test_smiles_encoded, torch.Tensor(np.array(X_test_copy)),torch.Tensor(np.array(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b85521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the code for GAT model\n",
    "N_dim = rev_X_train.shape[1]\n",
    "best_model=None\n",
    "valid_metrics_set = []\n",
    "cur_best_MAE = np.Inf\n",
    "for i in range(0,10):\n",
    "    #Split the data into 0.8 for training and rest for validation stuff\n",
    "    BATCH_SIZE = 256\n",
    "    train_dataset, valid_dataset = data_utils.random_split(X_train_smiles_graphs, [0.8, 0.2], generator = torch.Generator().manual_seed(i*42))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(X_test_smiles_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    #Build model parameters\n",
    "    CELL_INPUT_DIM = X_train_copy.shape[1]\n",
    "    CELL_OUT_DIM = 256\n",
    "    CELL_HID_DIMS = [1024,512]\n",
    "\n",
    "    SMILES_INPUT_DIM = 78\n",
    "    SMILES_N_HEAD = 2\n",
    "    SMILES_HID_DIM = 256\n",
    "    SMILES_OUT_DIM = 256\n",
    "\n",
    "    HID_DIM = 128\n",
    "    OUT_DIM = 1\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    cell_enc = NN_Encoder(CELL_INPUT_DIM, CELL_OUT_DIM, CELL_HID_DIMS, DROPOUT)\n",
    "    smiles_enc = GATNet(SMILES_INPUT_DIM, SMILES_N_HEAD, SMILES_HID_DIM, SMILES_OUT_DIM, DROPOUT)\n",
    "\n",
    "    #Make the model\n",
    "    model = Seq2Func_Net(cell_enc, smiles_enc, HID_DIM, OUT_DIM, DROPOUT, device=DEVICE).to(DEVICE)\n",
    "    \n",
    "    #Model training criterion\n",
    "    optimizer = optim.Adam(model.parameters(),weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    \n",
    "    inputfile_model = '../Models/gat_models/gat_supervised_checkpoint_'+str(i)+'.pt'\n",
    "    if (torch.cuda.is_available()):\n",
    "        model.load_state_dict(torch.load(inputfile_model))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(inputfile_model,map_location=torch.device('cpu')))\n",
    "        \n",
    "    valid_outputs = evaluation_net_performance(model, valid_loader, criterion, N_dim, DEVICE)\n",
    "    valid_metrics = calculate_regression_metrics(labels=np.array(valid_outputs[1]),predictions=np.array(valid_outputs[0]))\n",
    "    valid_metrics_set.append(valid_metrics)\n",
    "    if (valid_metrics[0]<cur_best_MAE):\n",
    "        cur_best_MAE = valid_metrics[0]\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a480957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    26.3122\n",
      "1    37.1113\n",
      "2     0.6433\n",
      "3     0.8020\n",
      "4     0.7996\n",
      "dtype: float64\n",
      "0    0.369915\n",
      "1    0.643790\n",
      "2    0.006865\n",
      "3    0.004190\n",
      "4    0.005719\n",
      "dtype: float64\n",
      "(40.885, 55.206, 0.392, 0.626, 0.619)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of the GAT model\n",
    "valid_metrics_df = pd.DataFrame(valid_metrics_set)\n",
    "mean_valid_metrics = pd.DataFrame.mean(valid_metrics_df,axis=0)\n",
    "print(mean_valid_metrics)\n",
    "sd_valid_metrics = pd.DataFrame.std(valid_metrics_df,axis=0)\n",
    "print(sd_valid_metrics)\n",
    "test_outputs = evaluation_net_performance(model, test_loader, criterion, N_dim, DEVICE)\n",
    "test_metrics = calculate_regression_metrics(labels=np.array(test_outputs[1]),predictions=np.array(test_outputs[0]))\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea7a3dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n",
      "Total parameters in model are:  2742273\n"
     ]
    }
   ],
   "source": [
    "best_cnn_model=None\n",
    "cnn_valid_metrics_set = []\n",
    "cur_best_MAE = np.Inf\n",
    "for i in range(0,10):\n",
    "    #Split the data into 0.8 for training and rest for validation stuff\n",
    "    BATCH_SIZE = 256\n",
    "    train_dataset, valid_dataset = data_utils.random_split(train, [0.8, 0.2], generator = torch.Generator().manual_seed(i*42))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    #Build model parameters\n",
    "    CELL_INPUT_DIM = X_train_copy.shape[1]\n",
    "    CELL_OUT_DIM = 256\n",
    "    CELL_HID_DIMS = [1024,512]\n",
    "\n",
    "    SMILES_INPUT_DIM = tokenizer.vocab_size\n",
    "    SMILES_ENC_EMB_DIM = 128\n",
    "    SMILES_OUT_DIM = 256\n",
    "\n",
    "    N_FILTERS = 64\n",
    "    FILTER_SIZES = [2,3,4,6,7,8,9,10]\n",
    "\n",
    "    HID_DIM = 256\n",
    "    OUT_DIM = 1\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    cell_enc = NN_Encoder(CELL_INPUT_DIM, CELL_OUT_DIM, CELL_HID_DIMS, DROPOUT)\n",
    "    smiles_enc = CNN_Encoder(SMILES_INPUT_DIM, SMILES_ENC_EMB_DIM, SMILES_OUT_DIM, N_FILTERS, FILTER_SIZES, DROPOUT)\n",
    "\n",
    "    #Make the model\n",
    "    model = Seq2Func(cell_enc, smiles_enc, HID_DIM, OUT_DIM, DROPOUT, device=DEVICE).to(DEVICE)\n",
    "    print(\"Total parameters in model are: \",count_parameters(model))\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    #Model training criterion\n",
    "    optimizer = optim.Adam(model.parameters(),weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    \n",
    "    inputfile_model = '../Models/cnn_models/cnn_supervised_checkpoint_'+str(i)+'.pt'\n",
    "    if (torch.cuda.is_available()):\n",
    "        model.load_state_dict(torch.load(inputfile_model))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(inputfile_model,map_location=torch.device('cpu')))\n",
    "                \n",
    "    cnn_valid_outputs = evaluation_performance(model, valid_loader, criterion, DEVICE)\n",
    "    cnn_valid_metrics = calculate_regression_metrics(labels=np.array(cnn_valid_outputs[1]),predictions=np.array(cnn_valid_outputs[0]))\n",
    "    cnn_valid_metrics_set.append(cnn_valid_metrics)\n",
    "    if (cnn_valid_metrics[0]<cur_best_MAE):\n",
    "        cur_best_MAE = cnn_valid_metrics[0]\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935b630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    26.0259\n",
      "1    37.0308\n",
      "2     0.6455\n",
      "3     0.8032\n",
      "4     0.7999\n",
      "dtype: float64\n",
      "0    0.227943\n",
      "1    0.489467\n",
      "2    0.006737\n",
      "3    0.004264\n",
      "4    0.004122\n",
      "dtype: float64\n",
      "(40.89, 55.543, 0.394, 0.627, 0.618)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of the GAT model\n",
    "cnn_valid_metrics_df = pd.DataFrame(cnn_valid_metrics_set)\n",
    "mean_cnn_valid_metrics = pd.DataFrame.mean(cnn_valid_metrics_df,axis=0)\n",
    "print(mean_cnn_valid_metrics)\n",
    "sd_cnn_valid_metrics = pd.DataFrame.std(cnn_valid_metrics_df,axis=0)\n",
    "print(sd_cnn_valid_metrics)\n",
    "cnn_test_outputs = evaluation_performance(model, test_loader, criterion, DEVICE)\n",
    "cnn_test_metrics = calculate_regression_metrics(labels=np.array(cnn_test_outputs[1]),predictions=np.array(cnn_test_outputs[0]))\n",
    "print(cnn_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb0cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n",
      "Total parameters in model are:  3195521\n"
     ]
    }
   ],
   "source": [
    "best_lstm_model=None\n",
    "lstm_valid_metrics_set = []\n",
    "cur_best_MAE = np.Inf\n",
    "for i in range(0,10):\n",
    "    #Split the data into 0.8 for training and rest for validation stuff\n",
    "    BATCH_SIZE = 256\n",
    "    train_dataset, valid_dataset = data_utils.random_split(train, [0.8, 0.2], generator = torch.Generator().manual_seed(i*42))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    #Build model parameters\n",
    "    CELL_INPUT_DIM = X_train_copy.shape[1]\n",
    "    CELL_OUT_DIM = 256\n",
    "    CELL_HID_DIMS = [1024,512]\n",
    "\n",
    "    SMILES_INPUT_DIM = tokenizer.vocab_size\n",
    "    SMILES_ENC_EMB_DIM = 128\n",
    "    SMILES_HID_DIM = 256\n",
    "    SMILES_OUT_DIM = 256\n",
    "\n",
    "    HID_DIM = 128\n",
    "    OUT_DIM = 1\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    cell_enc = NN_Encoder(CELL_INPUT_DIM, CELL_OUT_DIM, CELL_HID_DIMS, DROPOUT)\n",
    "    smiles_enc = LSTM_Encoder(SMILES_INPUT_DIM, SMILES_ENC_EMB_DIM, SMILES_HID_DIM, SMILES_OUT_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "    #Make the model\n",
    "    model = Seq2Func(cell_enc, smiles_enc, HID_DIM, OUT_DIM, DROPOUT, device=DEVICE).to(DEVICE)\n",
    "    print(\"Total parameters in model are: \",count_parameters(model))\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    #Model training criterion\n",
    "    optimizer = optim.Adam(model.parameters(),weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    \n",
    "    inputfile_model = '../Models/lstm_models/lstm_supervised_checkpoint_'+str(i)+'.pt'\n",
    "    if (torch.cuda.is_available()):\n",
    "        model.load_state_dict(torch.load(inputfile_model))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(inputfile_model,map_location=torch.device('cpu')))\n",
    "                \n",
    "    lstm_valid_outputs = evaluation_performance(model, valid_loader, criterion, DEVICE)\n",
    "    lstm_valid_metrics = calculate_regression_metrics(labels=np.array(lstm_valid_outputs[1]),predictions=np.array(lstm_valid_outputs[0]))\n",
    "    lstm_valid_metrics_set.append(lstm_valid_metrics)\n",
    "    if (lstm_valid_metrics[0]<cur_best_MAE):\n",
    "        cur_best_MAE = lstm_valid_metrics[0]\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de85c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    26.2674\n",
      "1    36.8707\n",
      "2     0.6447\n",
      "3     0.8029\n",
      "4     0.7990\n",
      "dtype: float64\n",
      "0    0.420244\n",
      "1    0.610936\n",
      "2    0.007558\n",
      "3    0.004654\n",
      "4    0.005558\n",
      "dtype: float64\n",
      "(40.92, 55.12, 0.383, 0.619, 0.611)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of the GAT model\n",
    "lstm_valid_metrics_df = pd.DataFrame(lstm_valid_metrics_set)\n",
    "mean_lstm_valid_metrics = pd.DataFrame.mean(lstm_valid_metrics_df,axis=0)\n",
    "print(mean_lstm_valid_metrics)\n",
    "sd_lstm_valid_metrics = pd.DataFrame.std(lstm_valid_metrics_df,axis=0)\n",
    "print(sd_lstm_valid_metrics)\n",
    "lstm_test_outputs = evaluation_performance(model, test_loader, criterion, DEVICE)\n",
    "lstm_test_metrics = calculate_regression_metrics(labels=np.array(lstm_test_outputs[1]),predictions=np.array(lstm_test_outputs[0]))\n",
    "print(lstm_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558f997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
